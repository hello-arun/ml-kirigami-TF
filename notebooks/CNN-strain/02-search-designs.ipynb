{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZG149sjfrvS",
        "outputId": "501abe12-6c61-4773-a6ce-0edd91ba40ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.9.1\n"
          ]
        }
      ],
      "source": [
        "# %tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import matplotlib\n",
        "# matplotlib.use('TKAgg')\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "random = np.random\n",
        "random.seed(142536)\n",
        "PROJECT_DIR = \"../../\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ejWJTvRsUIz"
      },
      "source": [
        "## Helper class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OJHFWAjkhHj0"
      },
      "outputs": [],
      "source": [
        "class Helper:\n",
        "    import numpy as np\n",
        "\n",
        "    def make_fine_grid(self,raw_data, n_corse_x = 3, n_corse_y = 5, n_fine_x = 30, n_fine_y = 80):\n",
        "        listFG = []  # List Fine Grid\n",
        "        N = len(raw_data)\n",
        "        for i in range(N):\n",
        "            print(f\"Pre Processing {i+1:05d}/{N}, {100*(i+1)//N}%\",end=\"\\r\",flush=True)\n",
        "            kirigami_config = raw_data[i, 0:15]\n",
        "            inner_wCuts = self.corse_to_fine_config(\n",
        "                kirigami_config, n_corse_x, n_corse_y, n_fine_x, n_fine_y)\n",
        "            listFG.append(inner_wCuts)\n",
        "\n",
        "        alldata_FG = np.array(listFG)\n",
        "        alldata_FG = np.append(alldata_FG, raw_data[:, -3:], 1)\n",
        "        return alldata_FG\n",
        "\n",
        "    def corse_to_fine_config(self,kirigami_config, n_corse_x, n_corse_y, n_fine_x, n_fine_y):\n",
        "        \"\"\"\n",
        "        Make Fine Grid using corse grid\n",
        "        0 5 10     0  80  160 ... 2320\n",
        "        1 6 11     .  .    .  ... .\n",
        "        2 7 12  => .  .    .  ... .\n",
        "        3 8 13     .  .    .  ... .\n",
        "        4 9 14     79 159 239 ... 2399\n",
        "\n",
        "        Parameters\n",
        "        --------------------\n",
        "        kirigami_config: Corse Kirigami config of size n_corse_x * n_corse_y\n",
        "        return: Fine grid 1D array of size n_fine_x*n_fine_y\n",
        "        \"\"\"\n",
        "        fine_grid = np.ones((n_fine_x,n_fine_y))\n",
        "        mx, my = n_fine_x//n_corse_x, n_fine_y//n_corse_y  # 10 16\n",
        "        zeros = np.array([1]*mx)[:,np.newaxis]\n",
        "        zeros[mx//3:2*mx//3+1]=0\n",
        "        # ONLY MAKE CUTS inside the INNER REGION !!\n",
        "        for index,num in enumerate(kirigami_config):\n",
        "            if num == 0:\n",
        "                i_corse_x = index // n_corse_y\n",
        "                i_corse_y = index % n_corse_y\n",
        "                fine_grid[mx*i_corse_x:mx*(i_corse_x+1),my*i_corse_y:my*(i_corse_y+1)] = zeros\n",
        "        return fine_grid.flatten()\n",
        "\n",
        "    def split_data(self,x, y, frac_training):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------------------------\n",
        "        x: numpy matrix \n",
        "        y: numpy array\n",
        "        percentage_test: float \n",
        "            percentage of data to be set for test set\n",
        "        Return\n",
        "        -------------------\n",
        "        X_train, X_valid, X_test, y_train, y_valid, y_test\n",
        "\n",
        "        \"\"\"\n",
        "        frac_valid = 1-frac_training\n",
        "        ntrain = int(frac_training * len(x))\n",
        "        nvalid = int(frac_valid * len(x))\n",
        "        # ntest = int(frac_valid * len(x))\n",
        "\n",
        "        X_train = x[:ntrain].reshape((-1,30,80,1))\n",
        "        X_valid = x[ntrain:ntrain+nvalid].reshape((-1,30,80,1))\n",
        "\n",
        "        y_train = y[:ntrain]\n",
        "        y_valid = y[ntrain:ntrain+nvalid]\n",
        "        # [:,np.newaxis] to convert it to column array\n",
        "        return X_train, X_valid, y_train[:,np.newaxis], y_valid[:,np.newaxis]\n",
        "helper = Helper()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S70acHFspzh"
      },
      "source": [
        "## Prepare Dataset\n",
        "We conver coarse grid data to fine grid data using helper class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dGaSVmR3gI56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre Processing 29791/29791, 100%\n",
            "Done..\n",
            "Rescaling Properties..\n",
            "Max Strain: 2.3053\n",
            "Max Stress: 107.9651\n",
            "Max Toughn: 52.3987\n",
            "Done..\n"
          ]
        }
      ],
      "source": [
        "alldata_15G = np.loadtxt(f'{PROJECT_DIR}/raw/alldata_15G.dat')\n",
        "alldata_FG  = helper.make_fine_grid(alldata_15G)\n",
        "print(\"\\nDone..\")\n",
        "# Rescale the data to make it convinent \n",
        "i_strain,i_toughness,i_stress=-3,-2,-1\n",
        "_max_strain = np.max(alldata_FG[:,i_strain])\n",
        "_max_toughness = np.max(alldata_FG[:,i_toughness])\n",
        "_max_stress = np.max(alldata_FG[:,i_stress])\n",
        "print(\"Rescaling Properties..\")\n",
        "print(\"Max Strain:\",_max_strain)\n",
        "print(\"Max Stress:\",_max_stress)\n",
        "print(\"Max Toughn:\",_max_toughness)\n",
        "alldata_FG[:,i_strain] /= _max_strain\n",
        "alldata_FG[:,i_stress] /= _max_stress\n",
        "alldata_FG[:,i_toughness] /= _max_toughness\n",
        "print(\"Done..\")\n",
        "# Shuffle the data althought not needed as it is already suffled\n",
        "# np.random.shuffle(alldata_FG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeOarrgEtfVi"
      },
      "source": [
        "## Basic Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((100, 2403), (29691, 2403))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_samples = 100  # Number of initial samples\n",
        "sample_indices = np.random.choice(len(alldata_FG), num_samples, replace=False)\n",
        "alldata_Train = alldata_FG[sample_indices]\n",
        "alldata_exluded = np.delete(alldata_FG,sample_indices,axis=0)\n",
        "alldata_Train.shape, alldata_exluded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC46CrU4tlWM",
        "outputId": "137e0969-375e-48cd-a0aa-8c4a3df1abcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No of featues: 2400 \n",
            "Property index: -3\n",
            "Training 90\n",
            "Validation 9\n",
            "Test Excluded 29691\n",
            "(90, 30, 80, 1) (90, 1)\n"
          ]
        }
      ],
      "source": [
        "__FEATURES = len(alldata_Train[0])-3\n",
        "__PROP_INDEX = i_strain  # -3: Strain, -1: Stress \n",
        "print(\"No of featues:\",__FEATURES,\"\\nProperty index:\",__PROP_INDEX)\n",
        "X_train, X_valid, y_train, y_valid = helper.split_data(alldata_Train[:,0:__FEATURES], alldata_Train[:,__PROP_INDEX], 0.9)\n",
        "X_test_ex, _, y_test_ex, _         = helper.split_data(alldata_exluded[:,0:__FEATURES], alldata_exluded[:,__PROP_INDEX], 1.0)\n",
        "print(\"Training\",len(X_train))\n",
        "print(\"Validation\",len(X_valid))\n",
        "print(\"Test Excluded\",len(X_test_ex))\n",
        "print(X_train.shape,y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuxttlngu3Kh"
      },
      "source": [
        "## Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7c5g_aqMu5dh"
      },
      "outputs": [],
      "source": [
        "n_hidden=64 # The best choice for hidden layer\n",
        "model = models.Sequential()\n",
        "# Filters:16, Kernal:3x3\n",
        "model.add(layers.Conv2D(\n",
        "    input_shape = (30,80,1),\n",
        "    filters = 16,\n",
        "    kernel_size = [3,3],\n",
        "    padding=\"same\",\n",
        "    activation=\"relu\"))\n",
        "# Max Pooling, kernel:2x2, strides:2 \n",
        "model.add(layers.MaxPooling2D((2,2),2))\n",
        "# Filters:32, Kernal:3x3\n",
        "model.add(layers.Conv2D(32,(3,3),activation=\"relu\"))\n",
        "# Max Pooling, kernel:2x2, strides:2 \n",
        "model.add(layers.MaxPooling2D((2,2),2))\n",
        "# Filters:64, Kernal:3x3\n",
        "model.add(layers.Conv2D(64,(3,3),activation=\"relu\"))\n",
        "# Max Pooling, kernel:2x2, strides:2 \n",
        "model.add(layers.MaxPooling2D((2,2),2))\n",
        "\n",
        "# Flatten for the fully connected layer\n",
        "model.add(layers.Flatten())\n",
        "# Fully connected layer 64:Neuron\n",
        "model.add(layers.Dense(n_hidden,activation=\"relu\"))\n",
        "model.add(layers.Dense(1,activation=\"linear\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17mNOZOyysvk",
        "outputId": "8fecd3e4-c2ac-44d2-a539-8e277a8dc40a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 80, 16)        160       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 40, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 38, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 19, 32)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 17, 64)         18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 2, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                65600     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 88,961\n",
            "Trainable params: 88,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33tWPlxnzNhz",
        "outputId": "4db96930-20a8-4e7b-f36e-8b60d1c558fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "9/9 [==============================] - 1s 29ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "Epoch 2/30\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "Epoch 3/30\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "Epoch 4/30\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "Epoch 5/30\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 6.5934e-04 - val_mean_squared_error: 6.5934e-04\n",
            "Epoch 6/30\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 9.6286e-04 - val_mean_squared_error: 9.6286e-04\n",
            "Epoch 7/30\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "Epoch 8/30\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 7.3837e-04 - val_mean_squared_error: 7.3837e-04\n",
            "Epoch 9/30\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "Epoch 10/30\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "Epoch 11/30\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "Epoch 12/30\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 7.5382e-04 - val_mean_squared_error: 7.5382e-04\n",
            "Epoch 13/30\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "Epoch 14/30\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 9.5955e-04 - val_mean_squared_error: 9.5955e-04\n",
            "Epoch 15/30\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "Epoch 16/30\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "Epoch 17/30\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "Epoch 18/30\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "Epoch 19/30\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "Epoch 20/30\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "Epoch 21/30\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "Epoch 22/30\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "Epoch 23/30\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "Epoch 24/30\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 8.6541e-04 - mean_squared_error: 8.6541e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "Epoch 25/30\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 7.8060e-04 - mean_squared_error: 7.8060e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "Epoch 26/30\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 7.8167e-04 - mean_squared_error: 7.8167e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "Epoch 27/30\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 8.0911e-04 - mean_squared_error: 8.0911e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "Epoch 28/30\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 4.2044e-04 - mean_squared_error: 4.2044e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "Epoch 29/30\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 3.2617e-04 - mean_squared_error: 3.2617e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "Epoch 30/30\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 4.0045e-04 - mean_squared_error: 4.0045e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./models/02-search/f16-f32-f64-h64\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./models/02-search/f16-f32-f64-h64\\assets\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_squared_error'])\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=num_samples//10,epochs=30, \n",
        "                    validation_data=(X_valid, y_valid))\n",
        "model.save(f\"./models/02-search/f16-f32-f64-h{n_hidden}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Find Best 100 Performers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "928/928 [==============================] - 6s 6ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(29691,)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = model.predict(X_test_ex).flatten()\n",
        "y_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[24297],\n",
              "       [    0],\n",
              "       [    2],\n",
              "       ...,\n",
              "       [27600],\n",
              "       [  846],\n",
              "       [ 4253]], dtype=int64)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find largest 100 indices\n",
        "\n",
        "new_sample_indices = np.argpartition(y_pred,-num_samples,axis=0)[-num_samples]\n",
        "newdata_Train = alldata_exluded[new_sample_indices]\n",
        "alldata_Train = np.append(alldata_Train,newdata_Train,axis=0)\n",
        "alldata_exluded = np.delete(alldata_exluded,new_sample_indices,axis=0)\n",
        "alldata_Train.shape, alldata_exluded.shape\n",
        "\n",
        "# Now remodel with this dataset and continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "CEYi-tA11E_h",
        "outputId": "a6d4d5ec-2bc3-4ca6-e39a-e27463c58497"
      },
      "outputs": [],
      "source": [
        "mse = tf.keras.metrics.mean_squared_error(\n",
        "    y_test, model.predict(X_test)\n",
        ")\n",
        "print(np.mean(mse))\n",
        "y_pred = model.predict(X_test)\n",
        "xx = y_pred.flatten()\n",
        "yy = y_test.flatten()\n",
        "#plt.plot([0,1],[1,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "xx = y_pred.flatten()\n",
        "yy = y_test.flatten()\n",
        "fig,(ax1,ax2,ax3)=plt.subplots(1,3)\n",
        "fig.set_size_inches(12,4)\n",
        "ax1.plot(y_train.flatten(),model.predict(X_train).flatten(),\".\",label=\"Training\")\n",
        "ax2.plot(y_valid.flatten(),model.predict(X_valid).flatten(),\".\",label=\"Validation\")\n",
        "ax3.plot(y_test.flatten(),model.predict(X_test).flatten(),\".\",label=\"Testing\")\n",
        "\n",
        "for ax in (ax1,ax2,ax3):\n",
        "    ax.plot([0,1],[0,1],\"r--\",lw=2.0)\n",
        "    ax.legend()\n",
        "    ax.set_xlim([0,1])\n",
        "    ax.set_ylim([0,1])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9S70acHFspzh"
      ],
      "name": "CNN-regression.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 (conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "091a525c9d5df072626171aed611359ef4003a1f6bfb9b336d556df39ecfb8de"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
